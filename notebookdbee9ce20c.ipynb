{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-02T13:04:21.152604Z","iopub.execute_input":"2024-10-02T13:04:21.153295Z","iopub.status.idle":"2024-10-02T13:04:22.223926Z","shell.execute_reply.started":"2024-10-02T13:04:21.153246Z","shell.execute_reply":"2024-10-02T13:04:22.222954Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models as models\nimport torchvision.transforms as T\nfrom torch.utils.data import DataLoader,random_split\nfrom tqdm import tqdm\nimport torchvision.datasets as datasets\nimport random","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:04:38.936206Z","iopub.execute_input":"2024-10-02T13:04:38.937131Z","iopub.status.idle":"2024-10-02T13:04:43.436163Z","shell.execute_reply.started":"2024-10-02T13:04:38.937086Z","shell.execute_reply":"2024-10-02T13:04:43.435266Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed):\n    torch.manual_seed(seed)  # Set seed for CPU\n    torch.cuda.manual_seed_all(seed)  # Set seed for all GPUs\n    random.seed(seed)  # Set seed for Python's random module\n    np.random.seed(seed)  # Set seed for NumPy\n\nset_seed(42)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:04:50.346151Z","iopub.execute_input":"2024-10-02T13:04:50.347142Z","iopub.status.idle":"2024-10-02T13:04:50.359143Z","shell.execute_reply.started":"2024-10-02T13:04:50.347099Z","shell.execute_reply":"2024-10-02T13:04:50.357917Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# setting up hyperparams\nbatch_size = 128\nlr = 1e-4\nnum_epochs = 15\nearly_stopping_patience = 5","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:04:54.223376Z","iopub.execute_input":"2024-10-02T13:04:54.224117Z","iopub.status.idle":"2024-10-02T13:04:54.228780Z","shell.execute_reply.started":"2024-10-02T13:04:54.224074Z","shell.execute_reply":"2024-10-02T13:04:54.227592Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"trans = T.Compose([\n    T.ToTensor(),\n    T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:04:54.710722Z","iopub.execute_input":"2024-10-02T13:04:54.711585Z","iopub.status.idle":"2024-10-02T13:04:54.716274Z","shell.execute_reply.started":"2024-10-02T13:04:54.711546Z","shell.execute_reply":"2024-10-02T13:04:54.715202Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_data = datasets.CIFAR10(root = \"/data\", train = True, download = True, transform = trans)\ntest_data = datasets.CIFAR10(root = \"/data\", train = False, download = True, transform = trans)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:04:55.105334Z","iopub.execute_input":"2024-10-02T13:04:55.105726Z","iopub.status.idle":"2024-10-02T13:05:03.737252Z","shell.execute_reply.started":"2024-10-02T13:04:55.105689Z","shell.execute_reply":"2024-10-02T13:05:03.736458Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /data/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:04<00:00, 35132553.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting /data/cifar-10-python.tar.gz to /data\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"train_size = int(0.8 * len(train_data))  \nval_size = len(train_data) - train_size \n\n# Split the dataset\ntrain_set, val_set = random_split(train_data, [train_size, val_size])\n\n# Create DataLoaders for train, val and test\n\ntrain_dl = DataLoader(train_set, batch_size=batch_size, shuffle=True)\nval_dl = DataLoader(val_set, batch_size=batch_size, shuffle=True)\ntest_dl = DataLoader(test_data, batch_size = batch_size, shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:05:26.686480Z","iopub.execute_input":"2024-10-02T13:05:26.687269Z","iopub.status.idle":"2024-10-02T13:05:26.720732Z","shell.execute_reply.started":"2024-10-02T13:05:26.687228Z","shell.execute_reply":"2024-10-02T13:05:26.719799Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model = models.wide_resnet50_2(pretrained = True)\nmodel.fc = nn.Linear(model.fc.in_features, 10)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:05:29.887021Z","iopub.execute_input":"2024-10-02T13:05:29.887411Z","iopub.status.idle":"2024-10-02T13:05:33.587752Z","shell.execute_reply.started":"2024-10-02T13:05:29.887362Z","shell.execute_reply":"2024-10-02T13:05:33.586852Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Wide_ResNet50_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet50_2_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth\" to /root/.cache/torch/hub/checkpoints/wide_resnet50_2-95faca4d.pth\n100%|██████████| 132M/132M [00:02<00:00, 59.9MB/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:05:33.589511Z","iopub.execute_input":"2024-10-02T13:05:33.589977Z","iopub.status.idle":"2024-10-02T13:05:33.649036Z","shell.execute_reply.started":"2024-10-02T13:05:33.589929Z","shell.execute_reply":"2024-10-02T13:05:33.648018Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:05:36.209982Z","iopub.execute_input":"2024-10-02T13:05:36.210615Z","iopub.status.idle":"2024-10-02T13:05:36.486708Z","shell.execute_reply.started":"2024-10-02T13:05:36.210576Z","shell.execute_reply":"2024-10-02T13:05:36.485634Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr = lr)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:05:37.503023Z","iopub.execute_input":"2024-10-02T13:05:37.503428Z","iopub.status.idle":"2024-10-02T13:05:37.509339Z","shell.execute_reply.started":"2024-10-02T13:05:37.503372Z","shell.execute_reply":"2024-10-02T13:05:37.508379Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"best_loss = np.inf\npatience_counter = 0","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:05:42.006687Z","iopub.execute_input":"2024-10-02T13:05:42.007385Z","iopub.status.idle":"2024-10-02T13:05:42.011587Z","shell.execute_reply.started":"2024-10-02T13:05:42.007345Z","shell.execute_reply":"2024-10-02T13:05:42.010593Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    train_correct = 0\n    train_total = 0\n    for inputs, labels in tqdm(train_dl, desc = f\"Epoch {epoch+1}/{num_epochs}\"):\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        _,pred_train = torch.max(outputs,1)\n        train_total += labels.size(0)\n        train_correct += (pred_train == labels).sum().item()\n        running_loss += loss.item()\n        \n        loss.backward()\n        optimizer.step()\n        \n    avg_loss = running_loss / len(train_dl)\n    train_acc = (train_correct/train_total)*100\n    print(f\"Train Loss : {avg_loss:.4f}   Train Accuracy : {train_acc:.4f}\")   \n    \n    \n    model.eval()\n    val_loss = 0.0\n    val_correct = 0\n    val_total = 0\n    for inputs, labels in tqdm(val_dl, desc = f\"Epoch {epoch+1}/{num_epochs}\"):\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        _, pred_test = torch.max(outputs, 1)\n        val_total += labels.size(0)\n        val_correct += (pred_test == labels).sum().item()\n        val_loss += loss.item()\n        \n    avg_val_loss = val_loss / len(val_dl)\n    test_acc = (val_correct/val_total)*100\n    print(f\"Val Loss : {avg_val_loss:.4f}   VAl Accuracy : {test_acc:.4f}\")\n    \n    if avg_val_loss < best_loss:\n        best_loss = avg_val_loss\n        patience_counter = 0\n        torch.save(model.state_dict(), 'plain_wide_resnet.pth')  # Save the best model\n    else:\n        patience_counter += 1\n        if patience_counter >= early_stopping_patience:\n            print(\"Early stopping triggered!\")\n            break","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:06:35.904603Z","iopub.execute_input":"2024-10-02T13:06:35.905443Z","iopub.status.idle":"2024-10-02T13:12:35.032357Z","shell.execute_reply.started":"2024-10-02T13:06:35.905392Z","shell.execute_reply":"2024-10-02T13:12:35.031477Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"Epoch 1/15: 100%|██████████| 313/313 [00:43<00:00,  7.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss : 0.5782   Train Accuracy : 80.7975\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/15: 100%|██████████| 79/79 [00:05<00:00, 15.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss : 0.5277   VAl Accuracy : 81.8300\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/15: 100%|██████████| 313/313 [00:45<00:00,  6.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss : 0.2885   Train Accuracy : 90.3600\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/15: 100%|██████████| 79/79 [00:05<00:00, 15.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss : 0.5276   VAl Accuracy : 82.8900\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/15: 100%|██████████| 313/313 [00:46<00:00,  6.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss : 0.1753   Train Accuracy : 94.0900\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/15: 100%|██████████| 79/79 [00:05<00:00, 15.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss : 0.5803   VAl Accuracy : 82.9500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/15: 100%|██████████| 313/313 [00:46<00:00,  6.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss : 0.1299   Train Accuracy : 95.7050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/15: 100%|██████████| 79/79 [00:05<00:00, 15.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss : 0.5991   VAl Accuracy : 83.4700\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/15: 100%|██████████| 313/313 [00:46<00:00,  6.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss : 0.1009   Train Accuracy : 96.6425\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/15: 100%|██████████| 79/79 [00:05<00:00, 15.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss : 0.6079   VAl Accuracy : 83.6800\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/15: 100%|██████████| 313/313 [00:46<00:00,  6.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss : 0.0775   Train Accuracy : 97.4075\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/15: 100%|██████████| 79/79 [00:05<00:00, 15.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss : 0.6382   VAl Accuracy : 84.0300\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/15: 100%|██████████| 313/313 [00:46<00:00,  6.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss : 0.0754   Train Accuracy : 97.5900\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/15: 100%|██████████| 79/79 [00:05<00:00, 15.60it/s]","output_type":"stream"},{"name":"stdout","text":"Val Loss : 0.6437   VAl Accuracy : 84.1300\nEarly stopping triggered!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for inputs, labels in test_dl:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f\"Accuracy on test set: {100 * correct / total:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:14:24.183267Z","iopub.execute_input":"2024-10-02T13:14:24.184011Z","iopub.status.idle":"2024-10-02T13:14:29.207742Z","shell.execute_reply.started":"2024-10-02T13:14:24.183969Z","shell.execute_reply":"2024-10-02T13:14:29.206745Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Accuracy on test set: 83.16%\n","output_type":"stream"}]},{"cell_type":"code","source":"def check_model_size(model):\n    total_params = sum(p.numel() for p in model.parameters())\n    print(f\"Total number of parameters: {total_params}\")\n    model_size_mb = total_params * 4 / (1024 ** 2)  # Assuming float32 (4 bytes)\n    print(f\"Approximate model size: {model_size_mb:.2f} MB\")","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:14:33.462319Z","iopub.execute_input":"2024-10-02T13:14:33.462742Z","iopub.status.idle":"2024-10-02T13:14:33.468313Z","shell.execute_reply.started":"2024-10-02T13:14:33.462685Z","shell.execute_reply":"2024-10-02T13:14:33.467333Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"print(\"model size before pruning : \")\ncheck_model_size(model)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:14:36.102777Z","iopub.execute_input":"2024-10-02T13:14:36.103165Z","iopub.status.idle":"2024-10-02T13:14:36.108940Z","shell.execute_reply.started":"2024-10-02T13:14:36.103129Z","shell.execute_reply":"2024-10-02T13:14:36.108048Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"model size before pruning : \nTotal number of parameters: 66854730\nApproximate model size: 255.03 MB\n","output_type":"stream"}]},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:14:39.542357Z","iopub.execute_input":"2024-10-02T13:14:39.542744Z","iopub.status.idle":"2024-10-02T13:14:39.549916Z","shell.execute_reply.started":"2024-10-02T13:14:39.542708Z","shell.execute_reply":"2024-10-02T13:14:39.548967Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"print(\"model size after pruning : \")\ncount_parameters(model)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:14:41.595031Z","iopub.execute_input":"2024-10-02T13:14:41.595431Z","iopub.status.idle":"2024-10-02T13:14:41.604409Z","shell.execute_reply.started":"2024-10-02T13:14:41.595374Z","shell.execute_reply":"2024-10-02T13:14:41.603417Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"model size after pruning : \n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"66854730"},"metadata":{}}]},{"cell_type":"markdown","source":"### model_size = 255MB, model_accuracy_on_test_set = 83.16%","metadata":{}},{"cell_type":"markdown","source":"## implementing research paper for pruning","metadata":{}},{"cell_type":"code","source":"class PolicyNetwork(nn.Module):\n    def __init__(self, num_filters, hidden_size=128):\n        super(PolicyNetwork, self).__init__()\n        self.fc1 = nn.Linear(num_filters, hidden_size)\n        self.activation = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_filters)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.activation(x)\n        x = self.fc2(x)\n\n        return torch.sigmoid(x)  # Ensure output is between 0 and 1\n    ","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:40:14.118041Z","iopub.execute_input":"2024-10-02T13:40:14.118683Z","iopub.status.idle":"2024-10-02T13:40:14.125243Z","shell.execute_reply.started":"2024-10-02T13:40:14.118631Z","shell.execute_reply":"2024-10-02T13:40:14.124326Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"class PruningAgent:\n    def __init__(self, num_filters):\n        self.num_filters = num_filters\n        self.policy_net = PolicyNetwork(num_filters).to(device)\n        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=1e-3)\n        \n    def calculate_reward(self, p_hat, p_star, b, A_l):\n        reward_accuracy = b - (p_star - p_hat) / b  # Calculate reward based on performance drop\n        # reward accuracy will be negative if p_hat is greter than p_star\n\n        # Calculate efficiency term\n        C_A_l = A_l.sum()  # THis is the number of filters kept ie number of ones\n        if C_A_l.item() == 0:\n            reward_efficiency = 0  # If no filters are kept, efficiency reward is zero\n        else:\n            reward_efficiency = np.log(self.num_filters / C_A_l.item())  # Efficiency reward\n            # if C_A_l is less, then number of filters kept is less, effienciency is increased\n\n        # Total reward\n        reward = reward_accuracy * reward_efficiency\n        return reward\n    \n    def get_binary_actions(self, weights):\n        # a randomly intialized tensor consisting of 0s and 1s\n        # 1=> keep the filter, 0=> remove the filter\n        weights_flat = weights.view(weights.size(0), -1)\n        probs = self.policy_net(weights_flat)\n        actions = (torch.rand(weights.size(0), device=device) < probs).float()  # Sample actions\n        return actions","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:40:24.109367Z","iopub.execute_input":"2024-10-02T13:40:24.109814Z","iopub.status.idle":"2024-10-02T13:40:24.120002Z","shell.execute_reply.started":"2024-10-02T13:40:24.109775Z","shell.execute_reply":"2024-10-02T13:40:24.118947Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"def prune_layer(model, layer_id, A_l):\n    with torch.no_grad():\n        layer_name = layer[:-1]\n        layer = model._modules[layer_name]\n        A_l = A_l.to(device)\n\n        # If the layer is a Sequential block\n        if isinstance(layer, torch.nn.Sequential):\n            # Access Block\n            block = layer[int(layer_id[-1])]  \n            conv_layer = block.conv1\n            W_l = conv_layer.weight.data.to(device)\n        elif isinstance(layer, torch.nn.Conv2d):\n            # If it's directly a Conv2d layer\n            W_l = layer.weight.data.to(device)\n        elif isinstance(layer, Bottleneck):\n            # If it's a Bottleneck block, access the conv1 layer inside it\n            W_l = layer.conv1.weight.data.to(device)\n        else:\n            raise ValueError(f\"Layer type {type(layer)} not handled for pruning.\")\n\n        # Apply binary actions to prune weights\n        pruned_W_l = W_l * A_l.view(-1, 1, 1, 1)  # Apply binary actions\n        pruned_W_l = pruned_W_l.to(device)\n\n        # Update the weights in the layer\n        if isinstance(layer, torch.nn.Sequential):\n            conv_layer.weight.data = pruned_W_l\n        elif isinstance(layer, Bottleneck):\n            layer.conv1.weight.data = pruned_W_l\n        else:\n            layer.weight.data = pruned_W_l\n\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:40:26.487960Z","iopub.execute_input":"2024-10-02T13:40:26.488335Z","iopub.status.idle":"2024-10-02T13:40:26.497957Z","shell.execute_reply.started":"2024-10-02T13:40:26.488299Z","shell.execute_reply":"2024-10-02T13:40:26.496998Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"def fine_tune_model(model, train_loader, criterion, optimizer, num_epochs=5):\n    \"\"\"Fine-tune the pruned model to recover performance.\"\"\"\n    model.train()\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        \n        print(f\"Fine-tuning Epoch {epoch+1}/{num_epochs}, Loss: {running_loss:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:40:31.951019Z","iopub.execute_input":"2024-10-02T13:40:31.951432Z","iopub.status.idle":"2024-10-02T13:40:31.958307Z","shell.execute_reply.started":"2024-10-02T13:40:31.951383Z","shell.execute_reply":"2024-10-02T13:40:31.957460Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"def evaluate_accuracy(model, val_loader):\n    \"\"\"Evaluate the model accuracy on the validation set.\"\"\"\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    return correct / total","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:40:32.797790Z","iopub.execute_input":"2024-10-02T13:40:32.798216Z","iopub.status.idle":"2024-10-02T13:40:32.805718Z","shell.execute_reply.started":"2024-10-02T13:40:32.798175Z","shell.execute_reply":"2024-10-02T13:40:32.804577Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"from torchvision.models.resnet import Bottleneck  # Import Bottleneck","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:40:33.963083Z","iopub.execute_input":"2024-10-02T13:40:33.963821Z","iopub.status.idle":"2024-10-02T13:40:33.967685Z","shell.execute_reply.started":"2024-10-02T13:40:33.963777Z","shell.execute_reply":"2024-10-02T13:40:33.966771Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"def prune_layer_by_layer(model, agents, criterion, optimizer, train_loader, val_loader, baseline_acc, b, num_epochs=10):\n    model.train()\n    \n    # Iterate over each convolutional layer to prune\n    for layer_id, agent in agents.items():\n        print(f\"Pruning {layer_id}...\")\n        # Access the weights of the convolutional layer within a Sequential block\n        layer_name = layer_id[:-1]\n        x = int(layer_id[-1])\n        layer = model._modules[layer_name]\n        \n        if isinstance(layer, torch.nn.Sequential):\n            # If the layer is a Sequential container, access the first Conv2d layer\n            block = layer[x]  \n            conv_layer = block.conv1\n            W_l = conv_layer.weight.data.to(device) \n        elif isinstance(layer, Bottleneck):\n            # If the layer is a Bottleneck block, access its convolutional layers\n            conv_layer = layer.conv1\n            W_l = conv_layer.weight.data.to(device) \n        else:\n            # If it's not a Sequential container, directly access the weights\n            W_l = layer.weight.data.to(device)  \n        \n        A_l = agent.get_binary_actions(W_l)\n        \n        # Prune the current layer based on actions\n        prune_layer(model, layer_id, A_l)\n        \n        # Fine-tune the entire network after pruning the current layer\n        print(f\"Fine-tuning the model after pruning {layer_id}...\")\n        fine_tune_model(model, train_loader, criterion, optimizer, num_epochs=4)\n        \n        # Validate the pruned model\n        new_acc = evaluate_accuracy(model, val_loader)\n        print(f\"Accuracy after pruning {layer_name}: {new_acc * 100:.2f}%\")\n        \n        # Calculate the reward\n        reward = agent.calculate_reward(new_acc, baseline_acc, b, A_l)\n        print(f\"Reward for pruning {layer_name}: {reward:.4f}\")\n        \n        self.optimizer.zero_grad()\n        log_prob = torch.log(agent.policy_net(W_l))\n        loss = -reward * log_prob.sum()  # REINFORCE loss\n        loss.backward()\n        self.optimizer.step()\n\n    print(\"Pruning completed.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:40:34.843386Z","iopub.execute_input":"2024-10-02T13:40:34.843746Z","iopub.status.idle":"2024-10-02T13:40:34.854387Z","shell.execute_reply.started":"2024-10-02T13:40:34.843714Z","shell.execute_reply":"2024-10-02T13:40:34.853434Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"# Define a set of pruning agents for the convolutional layers\npruning_agents = {\n   # 'conv10': PruningAgent(num_filters=model.conv1.weight.size(0)),\n    'layer10': PruningAgent(num_filters=model.layer1[0].conv1.weight.size(0)),  # First block of layer1\n    'layer11': PruningAgent(num_filters=model.layer1[1].conv1.weight.size(0)), # Second block of layer1\n    'layer12': PruningAgent(num_filters=model.layer1[2].conv1.weight.size(0)),\n    'layer20': PruningAgent(num_filters=model.layer2[0].conv1.weight.size(0)),  # First block of layer2\n    'layer21': PruningAgent(num_filters=model.layer2[1].conv1.weight.size(0)),  \n    'layer22': PruningAgent(num_filters=model.layer2[2].conv1.weight.size(0)),  \n    'layer23': PruningAgent(num_filters=model.layer2[3].conv1.weight.size(0)),  \n    'layer30': PruningAgent(num_filters=model.layer3[0].conv1.weight.size(0)),  # First block of layer3\n    'layer31': PruningAgent(num_filters=model.layer3[1].conv1.weight.size(0)),\n    'layer32': PruningAgent(num_filters=model.layer3[2].conv1.weight.size(0)),\n    'layer33': PruningAgent(num_filters=model.layer3[3].conv1.weight.size(0)),\n    'layer34': PruningAgent(num_filters=model.layer3[4].conv1.weight.size(0)),\n    'layer35': PruningAgent(num_filters=model.layer3[5].conv1.weight.size(0)),\n    'layer40': PruningAgent(num_filters=model.layer4[0].conv1.weight.size(0)),  # First block of layer4\n    'layer41': PruningAgent(num_filters=model.layer4[1].conv1.weight.size(0)), \n    'layer42': PruningAgent(num_filters=model.layer4[2].conv1.weight.size(0))\n}\n","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:40:35.685438Z","iopub.execute_input":"2024-10-02T13:40:35.686384Z","iopub.status.idle":"2024-10-02T13:40:35.722345Z","shell.execute_reply.started":"2024-10-02T13:40:35.686337Z","shell.execute_reply":"2024-10-02T13:40:35.721290Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"optimizer2 = optim.Adam(model.parameters(), lr = lr)\nbaseline_acc = evaluate_accuracy(model, test_dl)\nb = 1  # Performance drop bound\nnum_epochs = 10","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:36:24.425328Z","iopub.execute_input":"2024-10-02T13:36:24.425723Z","iopub.status.idle":"2024-10-02T13:36:29.442260Z","shell.execute_reply.started":"2024-10-02T13:36:24.425686Z","shell.execute_reply":"2024-10-02T13:36:29.441440Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"prune_layer_by_layer(model, pruning_agents, criterion, optimizer2, train_dl, val_dl, baseline_acc, b, num_epochs)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:40:40.531716Z","iopub.execute_input":"2024-10-02T13:40:40.532427Z","iopub.status.idle":"2024-10-02T13:40:40.709711Z","shell.execute_reply.started":"2024-10-02T13:40:40.532369Z","shell.execute_reply":"2024-10-02T13:40:40.708413Z"},"trusted":true},"execution_count":80,"outputs":[{"name":"stdout","text":"Pruning layer10...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprune_layer_by_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpruning_agents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline_acc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[78], line 25\u001b[0m, in \u001b[0;36mprune_layer_by_layer\u001b[0;34m(model, agents, criterion, optimizer, train_loader, val_loader, baseline_acc, b, num_epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# If it's not a Sequential container, directly access the weights\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     W_l \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mto(device)  \n\u001b[0;32m---> 25\u001b[0m A_l \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_binary_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW_l\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Prune the current layer based on actions\u001b[39;00m\n\u001b[1;32m     28\u001b[0m prune_layer(model, layer_id, A_l)\n","Cell \u001b[0;32mIn[73], line 27\u001b[0m, in \u001b[0;36mPruningAgent.get_binary_actions\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_binary_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, weights):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# a randomly intialized tensor consisting of 0s and 1s\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# 1=> keep the filter, 0=> remove the filter\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     weights_flat \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39mview(weights\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_flat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     actions \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mrand(weights\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;241m<\u001b[39m probs)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# Sample actions\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m actions\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[72], line 9\u001b[0m, in \u001b[0;36mPolicyNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m----> 9\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(x)\n\u001b[1;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (128x64 and 128x128)"],"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (128x64 and 128x128)","output_type":"error"}]},{"cell_type":"code","source":"print(\"model size after pruning : \")\ncheck_model_size(model)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T11:18:33.757482Z","iopub.execute_input":"2024-10-02T11:18:33.758210Z","iopub.status.idle":"2024-10-02T11:18:33.763589Z","shell.execute_reply.started":"2024-10-02T11:18:33.758168Z","shell.execute_reply":"2024-10-02T11:18:33.762645Z"},"trusted":true},"execution_count":157,"outputs":[{"name":"stdout","text":"model size after pruning : \nTotal number of parameters: 66854730\nApproximate model size: 255.03 MB\n","output_type":"stream"}]},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T11:19:38.184987Z","iopub.execute_input":"2024-10-02T11:19:38.185821Z","iopub.status.idle":"2024-10-02T11:19:38.190060Z","shell.execute_reply.started":"2024-10-02T11:19:38.185777Z","shell.execute_reply":"2024-10-02T11:19:38.189065Z"},"trusted":true},"execution_count":158,"outputs":[]},{"cell_type":"code","source":"print(\"model size after pruning : \")\ncount_parameters(model)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T11:19:52.051950Z","iopub.execute_input":"2024-10-02T11:19:52.052843Z","iopub.status.idle":"2024-10-02T11:19:52.060422Z","shell.execute_reply.started":"2024-10-02T11:19:52.052800Z","shell.execute_reply":"2024-10-02T11:19:52.059405Z"},"trusted":true},"execution_count":159,"outputs":[{"name":"stdout","text":"model size after pruning : \n","output_type":"stream"},{"execution_count":159,"output_type":"execute_result","data":{"text/plain":"66854730"},"metadata":{}}]},{"cell_type":"code","source":"model.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for inputs, labels in test_dl:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f\"Accuracy on test set: {100 * correct / total:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-10-02T11:20:36.534261Z","iopub.execute_input":"2024-10-02T11:20:36.534939Z","iopub.status.idle":"2024-10-02T11:20:41.455060Z","shell.execute_reply.started":"2024-10-02T11:20:36.534901Z","shell.execute_reply":"2024-10-02T11:20:41.454149Z"},"trusted":true},"execution_count":160,"outputs":[{"name":"stdout","text":"Accuracy on test set: 85.34%\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}